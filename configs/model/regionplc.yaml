_target_: src.models.networks.regionplc.litmodule.RegionPLCLitModule

optimizer:
  _target_: torch.optim.AdamW
  _partial_: true
  lr: 4e-3
  weight_decay: 1e-4

scheduler:
  _target_: torch.optim.lr_scheduler.OneCycleLR
  _partial_: true
  max_lr: ${model.optimizer.lr}
  pct_start: 0.39
  anneal_strategy: cos
  div_factor: 1
  final_div_factor: 10000.0

scheduler_interval: step

net:
  _target_: src.models.networks.regionplc.sparse_convnet.SparseConvUNetTextSeg
  _partial_: true

  backbone_cfg:
    in_channel: ${data.in_channel}
    mid_channel: 16
    block_reps: 2
    block_residual: true
    custom_sp1x1: true
    num_blocks: 7

  adapter_cfg:
    in_channel: ${model.net.backbone_cfg.mid_channel}
    text_channel: ${model.clip_encoder.text_dim}
    num_layers: 2
    last_norm: true

  binary_head_cfg:
    ignore_label: ${data.train_dataset.ignore_label}
    in_channel: ${model.net.backbone_cfg.mid_channel}
    block_reps: ${model.net.backbone_cfg.block_reps}
    block_residual: ${model.net.backbone_cfg.block_residual}
    num_blocks: ${model.net.backbone_cfg.num_blocks}
    binary_thresh: 0.5
    hook_feature_list:
      [
        "unet.blocks.block1",
        "unet.u.blocks.block1",
        "unet.u.u.blocks.block1",
        "unet.u.u.u.blocks.block1",
        "unet.u.u.u.u.blocks.block1",
        "unet.u.u.u.u.u.blocks.block1",
        "unet.u.u.u.u.u.u.blocks.block1",
      ]
    custom_sp1x1: true
    detach: true
    loss_weight: 1.0
    voxel_loss: false

loss_cfg:
  caption_loss:
    loss_reduction: weighted_sum
    use_logit_scale: true
  seg_loss:
    normalize_input: false
    text_clip_path: !!null
    loss_type: cross_entropy
    ignore_label: ${data.train_dataset.ignore_label}
    learnable_logit: false
  binary_loss: true
  binary_loss_weight: 1.0
  caption_loss_weight: 0.5
  seg_loss_weight: 1.0
  clip_learnable_logit_scale: false
  sync_dist: true
  train_clip_text_alignment: false
  train_clip_image_alignment: false
  clip_text_loss_weight: 1.0
  clip_image_loss_weight: 1.0

# compile model for faster training with pytorch 2.0
clip_encoder:
  model_id: ViT-B/16
  text_dim: 512

compile: false

eval_cfg:
  eval_clip_text_alignment: false
  eval_clip_image_alignment: false

use_prompt: false
