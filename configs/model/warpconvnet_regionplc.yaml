_target_: src.models.networks.warp.litmodule.WarpLitModule

optimizer:
  _target_: src.models.optimization.build_optimizer
  _partial_: true
  optim_cfg:
    LR: 4e-3
    SCHEDULER: adam_onecycle
    OPTIMIZER: adam_onecycle
    WEIGHT_DECAY: 0.0001
    MOMENTUM: 0.9
    STEP_EPOCH: 50
    MULTIPLIER: 0.1
    CLIP_GRAD: False
    PCT_START: 0.39
    DIV_FACTOR: 1
    MOMS: [0.95, 0.85]

scheduler:
  _target_: src.models.optimization.build_scheduler
  _partial_: true
  last_epoch: -1
  optim_cfg: ${model.optimizer.optim_cfg}

scheduler_interval: step

net:
  _target_: src.models.networks.warp.regionplc.RegionPLCToCLIP
  _partial_: true

  backbone_cfg:
    in_channel: ${data.in_channel}
    in_channels: [16, 32, 48, 64, 80, 96, 112, 128]
    out_channels: [16, 32, 48, 64, 80, 96, 112, 128]
    num_blocks: 2

  # last mlp
  adapter_cfg:
    in_channel: ${model.net.backbone_cfg.out_channels[0]}
    text_channel: 512
    num_layers: 2
    last_norm: true

loss_cfg:
  caption_loss:
    reduction: weighted_sum
    use_logit_scale: true
  seg_loss:
    normalize_input: false
    text_clip_path: !!null
    loss_type: cross_entropy
    ignore_label: ${data.train_dataset.ignore_label}
    learnable_logit: false
    eval_only: true
  binary_loss: false
  binary_loss_weight: 1.0
  caption_loss_weight: 1.0
  seg_loss_weight: 1.0
  clip_learnable_logit_scale: false
  sync_dist: true
  train_clip_text_alignment: false
  train_clip_image_alignment: false
  clip_text_loss_weight: 1.0
  clip_image_loss_weight: 1.0

# compile model for faster training with pytorch 2.0
clip_encoder:
  model_id: ViT-B/16
  text_dim: 512

compile: false

eval_cfg:
  eval_clip_text_alignment: false
  eval_clip_image_alignment: false

  seg_eval:
    normalize_input: ${model.loss_cfg.seg_loss.normalize_input}
    text_clip_path: ${model.loss_cfg.seg_loss.text_clip_path}

use_prompt: false
