<!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta http-equiv="Permissions-Policy" content="private-state-token-redemption=()" />
    <title>Mosaic3D: Foundation Dataset and Model for Open-Vocabulary 3D Segmentation</title>
    <!-- Add Bulma CSS -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma@0.9.4/css/bulma.min.css" />
    <!-- Add Font Awesome for icons -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/all.min.css" />
    <!-- Add PhotoSwipe -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/photoswipe@5.3.5/dist/photoswipe.css" />
    <script src="https://cdn.jsdelivr.net/npm/photoswipe@5.3.5/dist/photoswipe.min.js"></script>
    <style>
      .hero {
        background: #f6f8fa;
      }

      .publication-title {
        font-family: 'Google Sans', sans-serif;
      }

      .publication-authors {
        color: #4a4a4a;
      }

      .publication-banner {
        max-height: 300px;
        width: 100%;
        object-fit: cover;
      }

      .publication-header .hero-body {
        padding-bottom: 3rem;
        padding-top: 3rem;
      }

      .button.is-rounded {
        border-radius: 4px;
      }

      .gallery {
        scrollbar-width: thin;
        scrollbar-color: #888 #f1f1f1;
      }

      .gallery::-webkit-scrollbar {
        height: 8px;
      }

      .gallery::-webkit-scrollbar-track {
        background: #f1f1f1;
        border-radius: 4px;
      }

      .gallery::-webkit-scrollbar-thumb {
        background: #888;
        border-radius: 4px;
      }

      .gallery::-webkit-scrollbar-thumb:hover {
        background: #555;
      }
    </style>
  </head>

  <body>
    <!-- Hero -->
    <section class="hero">
      <div class="hero-body">
        <div class="container has-text-centered">
          <div class="level">
            <div class="level-item">
              <figure class="image is-64x64 mr-4">
                <img
                  src="./static/images/logo-64.png"
                  srcset="
                    ./static/images/logo-64.png   64w,
                    ./static/images/logo-128.png 128w,
                    ./static/images/logo-256.png 256w
                  "
                  sizes="(max-width: 768px) 64px,
                                        128px"
                  alt="Mosaic3D Logo"
                />
              </figure>
              <h1 class="title publication-title is-1 has-text-black">Mosaic3D</h1>
            </div>
          </div>
          <h2 class="title publication-title is-2 has-text-black">
            Foundation Dataset and Model for Open-Vocabulary 3D Segmentation
          </h2>

          <div class="publication-authors">
            <span class="author-block"> Junha Lee<sup>1,2</sup>, </span>
            <span class="author-block"> Chunghyun Park<sup>1,2</sup>, </span>
            <span class="author-block"> Jaesung Choe<sup>1</sup>, </span>
            <span class="author-block"> Frank Wang<sup>1</sup>, </span>
            <span class="author-block"> Jan Kautz<sup>1</sup>, </span>
            <span class="author-block"> Minsu Cho<sup>2</sup>, </span>
            <span class="author-block"> Chris Choy<sup>1</sup> </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>NVIDIA</span>
            <span class="author-block"><sup>2</sup>KAIST</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <span class="link-block">
                <a href="#" class="button is-medium is-primary">
                  <span class="icon">
                    <i class="fas fa-file-alt"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="#" class="button is-medium is-info">
                  <span class="icon">
                    <i class="fab fa-github"></i>
                  </span>
                  <span>Code (Coming Soon)</span>
                </a>
              </span>
              <span class="link-block">
                <a href="#" class="button is-medium is-success">
                  <span class="icon">
                    <i class="fas fa-database"></i>
                  </span>
                  <span>Data (Coming Soon)</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </section>

    <!-- Content -->
    <section class="section">
      <div class="container">
        <!-- TLDR -->
        <div class="columns is-centered has-text-centered">
          <div class="column is-four-fifths">
            <h2 class="title is-3">TLDR</h2>
            <div class="content has-text-justified">
              <ul>
                <li>Propose a large-scale dataset of open vocabulary 3D mask-text pairs.</li>
                <li>Propose a foundation model for open-vocabulary 3D segmentation.</li>
              </ul>
            </div>
          </div>
        </div>

        <!-- Results -->
        <div class="columns is-centered has-text-centered">
          <div class="column is-four-fifths">
            <h2 class="title is-3">Dataset Visualization</h2>
            <div class="gallery" style="width: 100%; overflow-x: auto; white-space: nowrap; padding: 20px 0">
              <a
                href="./static/images_dataset/medium/render_aerial_max_x.png"
                style="display: inline-block; margin-right: 20px"
                data-pswp-width="1200"
                data-pswp-height="800"
              >
                <img
                  src="./static/images_dataset/thumbs/render_aerial_max_x.png"
                  style="height: 300px; width: auto"
                  class="lazyload"
                  alt="Dataset Example 1"
                />
              </a>
              <a
                href="./static/images_dataset/medium/render_aerial_min_y.png"
                style="display: inline-block; margin-right: 20px"
                data-pswp-width="1200"
                data-pswp-height="800"
              >
                <img
                  src="./static/images_dataset/thumbs/render_aerial_min_y.png"
                  style="height: 300px; width: auto"
                  class="lazyload"
                  alt="Dataset Example 2"
                />
              </a>
              <a
                href="./static/images_dataset/medium/scene0401_00_render_aerial_max_y.png"
                data-pswp-width="1200"
                data-pswp-height="800"
              >
                <img
                  src="./static/images_dataset/thumbs/scene0401_00_render_aerial_max_y.png"
                  class="lazyload"
                  alt="Dataset Example 3"
                />
              </a>
              <a
                href="./static/images_dataset/medium/scene0625_01_render_aerial_max_y.png"
                data-pswp-width="1200"
                data-pswp-height="800"
              >
                <img
                  src="./static/images_dataset/thumbs/scene0625_01_render_aerial_max_y.png"
                  class="lazyload"
                  alt="Dataset Example 4"
                />
              </a>
            </div>
          </div>
        </div>

        <!-- Abstract -->
        <div class="columns is-centered has-text-centered">
          <div class="column is-four-fifths">
            <h2 class="title is-3">Abstract</h2>
            <div class="content has-text-justified">
              <p>
                We tackle open-vocabulary 3D scene understanding by introducing a novel data generation pipeline and
                training framework. Our method addresses three critical requirements for effective training: precise 3D
                region segmentation, comprehensive textual descriptions, and sufficient dataset scale. By leveraging
                state-of-the-art open-vocabulary image segmentation models and region-aware Vision-Language Models
                (VLM), we develop an automatic pipeline that generates high-quality 3D mask-text pairs. Applying this
                pipeline to multiple 3D scene datasets, we create the Mosaic3D dataset, a dataset of over 30K annotated
                scenes with 5.6M mask-text pairsâ€”significantly larger than existing datasets. Building upon this data,
                we propose Mosaic3D, a foundation model combining a 3D encoder trained with contrastive learning and a
                lightweight mask decoder for open-vocabulary 3D semantic and instance segmentation. Our approach
                achieves state-of-the-art results on open-vocabulary 3D semantic and instance segmentation tasks
                including ScanNet200, Matterport3D, and ScanNet++, with ablation studies validating the effectiveness of
                our large-scale training data.
              </p>
            </div>
          </div>
        </div>

        <!-- Citation -->
        <div class="columns is-centered has-text-centered">
          <div class="column is-four-fifths">
            <h2 class="title is-3">Citation</h2>
            <div class="content has-text-justified">
              <pre><code>@article{lee2025mosaic3d,
    title={Mosaic3D: Foundation Dataset and Model for Open-Vocabulary 3D Segmentation},
    author={Junha Lee and Chunghyun Park and Jaesung Choe and Frank Wang and Jan Kautz and Minsu Cho and Chris Choy},
    journal={arXiv},
    year={2025}
}</code></pre>
            </div>
          </div>
        </div>
      </div>
    </section>

    <!-- Initialize PhotoSwipe -->
    <script>
      import PhotoSwipeLightbox from 'photoswipe/lightbox';
      const lightbox = new PhotoSwipeLightbox({
        gallery: '.gallery',
        children: 'a',
        pswpModule: PhotoSwipe,
      });
      lightbox.init();
    </script>
  </body>
</html>
